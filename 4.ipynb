{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+9jCkfu0IVGY/cxvyLHz6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kosirobwada/DeepLearningFromZero2/blob/main/4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 第4章 word2vecの改良"
      ],
      "metadata": {
        "id": "fbV4D6f_hTFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding層の実装\n",
        "\n",
        "・扱うコーパスが増加すると、単語のone-hot表現の次元数が増える。\n",
        "\n",
        "・これにより、巨大なベクトルと重み行列の積を取らなければいけなくなる。\n",
        "\n",
        "・これを解決するために、Embedding層の追加。\n",
        "\n",
        "・行列から行を抜き出す処理を行う。"
      ],
      "metadata": {
        "id": "uRRDNzAWiCFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "W = np.arange(21).reshape(7,3)\n",
        "print(W)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vycJMH8hc3C",
        "outputId": "cb8d0a47-013a-4661-bae0-602a61129818"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  1  2]\n",
            " [ 3  4  5]\n",
            " [ 6  7  8]\n",
            " [ 9 10 11]\n",
            " [12 13 14]\n",
            " [15 16 17]\n",
            " [18 19 20]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.array([1,0,3,0])\n",
        "print(W[idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drAHLda1jBbL",
        "outputId": "76295a80-fd5e-4501-dd4d-ef06229e9c09"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 3  4  5]\n",
            " [ 0  1  2]\n",
            " [ 9 10 11]\n",
            " [ 0  1  2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding:\n",
        "  def __init__(self, W):\n",
        "    self.params = [W]\n",
        "    self.grads = [np.zeros_like(W)]\n",
        "    self.idx = None\n",
        "\n",
        "  def forward(self, idx):\n",
        "    W, = self.params\n",
        "    self.idx = idx\n",
        "    out = W[idx]\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dw, = self.grads\n",
        "    dw[...] = 0\n",
        "\n",
        "    for i, word_id in enumerate(self.idx):\n",
        "      dw[word_id] += dout[i]\n",
        "    return None"
      ],
      "metadata": {
        "id": "gyqilesLjKMN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Negative Sampling\n",
        "\n",
        "・中間層以降の行列の積とSoftMax層の計算がボトルネック。\n",
        "\n",
        "・これを解決する策としてNegative Sampling（負例サンプリング）\n",
        "\n",
        "・SoftMaxの代わりに、Negative Samplingを用いることで、語彙数がどれだけ多くなったとしても、計算量を抑えることができる。"
      ],
      "metadata": {
        "id": "tz2gA-6BmP8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingDot:\n",
        "  def __init__(self, W):\n",
        "    self.embed = Embedding(W)\n",
        "    self.params = self.embed.params\n",
        "    self.grads = self.embed.grads\n",
        "    self.cache = None\n",
        "\n",
        "  def forward(self, h, idx):\n",
        "    target_W = self.embed.forward(idx)\n",
        "    out = np.sum(target_W = h, axis=1)\n",
        "    self.cache = (h,target_W)\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    h, target_W = self.cache\n",
        "    dout = dout.reshape(dout.shape[0], 1)\n",
        "\n",
        "    dtarget_W = dout * h\n",
        "    self.embed.backward(dtarget_W)\n",
        "    dh = dout * target_W\n",
        "    return dh"
      ],
      "metadata": {
        "id": "BcuxQjgRmtCO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 確率分布\n",
        "\n",
        "・Negative Samplingする際に、負例をどのようにサンプリングするか？\n",
        "\n",
        "・コーパスの中で、抽出されやすい語はサンプリングされやすくし、抽出されにくい語はサンプリングされにくくする。\n",
        "\n",
        "・コーパスの中で単語の使用頻度に応じてサンプリングするには、コーパスから各単語の出現頻度を求め、確率分布で表す。\n",
        "\n"
      ],
      "metadata": {
        "id": "8teneQgsrdMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.choice(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGAjmJiGqwex",
        "outputId": "a974e06b-3985-4995-f5be-3b84e7197cbd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.choice(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36CMKrOjqzx0",
        "outputId": "64f90f73-5df7-4773-ec06-3638b9a67af8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['you','say','goodbye','I','say','hello']\n",
        "print(np.random.choice(words))\n",
        "print(np.random.choice(words))\n",
        "print(np.random.choice(words, size=5))\n",
        "print(np.random.choice(words, size=5, replace=False))\n",
        "\n",
        "p = [0.5,0.1,0.05,0.2,0.05,0.1]\n",
        "print(np.random.choice(words, p=p))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nvmkSyoq5Ob",
        "outputId": "ae640ba1-199c-4056-d5c3-a9e2f4985b51"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "say\n",
            "I\n",
            "['you' 'say' 'you' 'I' 'say']\n",
            "['goodbye' 'you' 'say' 'hello' 'say']\n",
            "you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "元の確率分布を0.75乗する。"
      ],
      "metadata": {
        "id": "7gOu7id8tB1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![スクリーンショット 2023-08-11 220526.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "X2_3Mrcbs2_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = [0.7, 0.29, 0.01]\n",
        "new_p = np.power(p, 0.75)\n",
        "print(new_p)\n",
        "\n",
        "new_p /= np.sum(new_p)\n",
        "print(new_p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "982I5xF0tQxZ",
        "outputId": "bd51572c-e7d4-46d1-b01e-a64f1e360e80"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.76528558 0.39518322 0.03162278]\n",
            "[0.64196878 0.33150408 0.02652714]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "class UnigramSampler:\n",
        "    def __init__(self, corpus, power, sample_size):\n",
        "        self.sample_size = sample_size\n",
        "        self.vocab_size = None\n",
        "        self.word_p = None\n",
        "\n",
        "        counts = collections.Counter()\n",
        "\n",
        "        for word_id in corpus:\n",
        "            counts[word_id] += 1\n",
        "\n",
        "        vocab_size = len(counts)\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.word_p = np.zeros(vocab_size)\n",
        "\n",
        "        for i in range(vocab_size):\n",
        "            self.word_p[i] = counts[i]\n",
        "\n",
        "        self.word_p = np.power(self.word_p, power)\n",
        "        self.word_p /= np.sum(self.word_p)\n",
        "    def get_negative_sample(self, target):\n",
        "        batch_size = target.shape[0]\n",
        "\n",
        "        # this code doesn't apply CuPy because I don't have Nvidia GPU\n",
        "        negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            p = self.word_p.copy()\n",
        "            target_idx = target[i]\n",
        "            p[target_idx] = 0\n",
        "            p /= p.sum()\n",
        "            negative_sample[i, :] = np.random.choice(\n",
        "                self.vocab_size,\n",
        "                size=self.sample_size,\n",
        "                replace=False,\n",
        "                p=p)\n",
        "\n",
        "        return negative_sample\n"
      ],
      "metadata": {
        "id": "q_J1ngCgvPu9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = np.array([0,1,2,3,4,1,2,3])\n",
        "power = 0.75\n",
        "sample_size = 2\n",
        "\n",
        "sampler = UnigramSampler(corpus, power, sample_size)\n",
        "target = np.array([1,3,0])\n",
        "negative_sample = sampler.get_negative_sample(target)\n",
        "print(negative_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51xN8bn2ucPF",
        "outputId": "a87f74ce-f485-466c-a5e5-8c53419cc380"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2 0]\n",
            " [4 1]\n",
            " [2 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "\n",
        "    # if training data is one-hot-vector\n",
        "    # then convert to teaching label's index\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "\n",
        "    batch_size = y.shape[0]\n",
        "\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
      ],
      "metadata": {
        "id": "gEjknUT4wpwV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SigmoidWithLoss:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.loss = None\n",
        "        self.y = None # sigmoid output\n",
        "        self.t = None # teaching data\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = 1 / (1 + np.exp(-x))\n",
        "\n",
        "        self.loss = cross_entropy_error(np.c_[1 - self.y, self.y], self.t)\n",
        "\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "\n",
        "        dx = (self.y - self.t) * dout / batch_size\n",
        "        return dx"
      ],
      "metadata": {
        "id": "xRI9QuPnwSTY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NegativeSamplingLoss:\n",
        "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
        "        self.sample_size = sample_size\n",
        "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
        "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n",
        "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
        "        self.params, self.grads = [], []\n",
        "\n",
        "        for layer in self.embed_dot_layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def forward(self, h, target):\n",
        "        batch_size = target.shape[0]\n",
        "        negative_sample = self.sampler.get_negative_sample(target)\n",
        "\n",
        "        # positive example forward\n",
        "        score = self.embed_dot_layers[0].forward(h, target)\n",
        "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
        "        loss = self.loss_layers[0].forward(score, correct_label)\n",
        "\n",
        "        # negative example forward\n",
        "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
        "\n",
        "        for i in range(self.sample_size):\n",
        "            negative_target = negative_sample[:, i]\n",
        "            score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
        "            loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dh = 0\n",
        "\n",
        "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
        "            dscore = l0.backward(dout)\n",
        "            dh += l1.backward(dscore)\n",
        "\n",
        "        return dh"
      ],
      "metadata": {
        "id": "5KSmvxFRv3q4"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}